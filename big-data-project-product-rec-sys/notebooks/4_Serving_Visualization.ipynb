{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c656dc-84ee-49df-82c8-32ff3ca0e785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define your project structure names\n",
    "catalog_name = \"big_data_project\"\n",
    "gold_schema_name = \"gold\"\n",
    "silver_schema_name = \"silver\" # We also need this for visualizations\n",
    "\n",
    "# Define the full table names\n",
    "gold_table = f\"{catalog_name}.{gold_schema_name}.products_with_features\"\n",
    "silver_table = f\"{catalog_name}.{silver_schema_name}.products_cleaned\"\n",
    "\n",
    "# Load our gold data\n",
    "gold_df = spark.table(gold_table)\n",
    "\n",
    "print(f\"Successfully loaded Gold table: {gold_table}\")\n",
    "display(gold_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30630d9e-08a3-43e0-8703-81e4625bbb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Configure the LSH model to work with our \"features\" column\n",
    "brp = BucketedRandomProjectionLSH(\n",
    "    inputCol=\"features\",       # The ML vector column from our Gold table\n",
    "    outputCol=\"hashes\",        # A new column for hash values\n",
    "    bucketLength=2.0,\n",
    "    numHashTables=3\n",
    ")\n",
    "\n",
    "# 2. Fit the LSH model to our data\n",
    "lsh_model = brp.fit(gold_df)\n",
    "\n",
    "# 3. Transform our data to add the 'hashes' column\n",
    "# We remove .cache() because it is not supported on Serverless\n",
    "lsh_df = lsh_model.transform(gold_df)\n",
    "\n",
    "print(\"LSH model is ready for finding similar items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753797c2-61c1-49a7-a503-d378eb0ae6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "def get_recommendations_by_name(target_name_query, num_recs=5):\n",
    "    \"\"\"\n",
    "    Finds a product by a search query and then finds similar products.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Find the product by its name (using a 'like' query to find a partial match)\n",
    "    print(f\"Searching for product matching: '{target_name_query}'...\")\n",
    "    \n",
    "    target_product = lsh_df.filter(\n",
    "        lower(col(\"title\")).like(f\"%{target_name_query.lower()}%\")\n",
    "    ).select(\"features\", \"title\", \"asin\").first()\n",
    "    \n",
    "    # 2. Check if we found a product\n",
    "    if not target_product:\n",
    "        print(f\"No product found with a name like '{target_name_query}'.\")\n",
    "        return None\n",
    "    \n",
    "    target_vector = target_product.features\n",
    "    target_asin = target_product.asin\n",
    "    \n",
    "    print(f\"Found base product: {target_product.title} (ASIN: {target_asin})\")\n",
    "    print(\"--- Finding similar items... ---\")\n",
    "    \n",
    "    # 3. Use the LSH model to find approximate nearest neighbors\n",
    "    similar_products_df = lsh_model.approxNearestNeighbors(\n",
    "        lsh_df,\n",
    "        target_vector,\n",
    "        num_recs + 1  # Get N+1 because it will find itself\n",
    "    )\n",
    "    \n",
    "    # 4. Filter out the original product (it will find itself)\n",
    "    results_df = similar_products_df.filter(col(\"asin\") != target_asin)\n",
    "    \n",
    "    # 5. Select and display the results\n",
    "    final_recs = results_df.select(\n",
    "        \"asin\",\n",
    "        \"title\",\n",
    "        \"price\",\n",
    "        \"stars\",\n",
    "        \"distCol\"  # Lower distance is more similar\n",
    "    ).orderBy(\"distCol\").limit(num_recs)\n",
    "    \n",
    "    return final_recs\n",
    "\n",
    "print(\"Recommendation function 'get_recommendations_by_name' is defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb2c8d9-e1e3-4349-b0f0-71abf83e3afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test the new function using a name query\n",
    "recs_df = get_recommendations_by_name(target_name_query=\"sion softside\", num_recs=5)\n",
    "\n",
    "if recs_df:\n",
    "    display(recs_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4_Serving_Visualization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
